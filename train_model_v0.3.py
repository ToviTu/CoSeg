# -*- coding: utf-8 -*-
"""CV_archi_0.3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gr39W8G0bbFAYmhD0iv6oi0xzx68xR48
"""

import torch
from torch import nn
from transformers import CLIPProcessor, CLIPModel, CLIPTextModelWithProjection, CLIPVisionModelWithProjection
import torch.nn.functional as F
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms
import torchvision.transforms.functional as TF
import os
import numpy as np
from torch.optim import Adam
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR
import tqdm

"""## Dataset"""

src_dir = "/home/research/jianhong.t/OpenVocab_Seg_with_AutoRegres/src/"
dataset_dir = "/scratch/t.tovi/datasets/"
image_dir = "COCO_stuff_images/train2017/"
annotation_dir = "COCO_stuff_annotations/train2017/"

class COCOStuffDataset(Dataset):
    def __init__(self, image_dir, annotation_dir, img_size=224):
        """
        Args:
            image_dir (string): Directory with all the images.
            annotation_dir (string): Directory with all the annotations.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.image_dir = image_dir
        self.annotation_dir = annotation_dir
        self.images = os.listdir(image_dir)
        self.img_size = img_size

        # Load the label mapping
        self.digit_to_object_mapping = {}
        with open(f'{src_dir}labels.txt', 'r') as file:
            for line in file:
                key, value = line.strip().split(':')
                self.digit_to_object_mapping[int(key)] = value.strip()
        self.digit_to_object_mapping[255] = "unlabled"

    def center_crop(self, image, mask):
        transform = transforms.CenterCrop(self.img_size)
        return transform(image), transform(mask)

    def resize(self, image, mask):
        transform = transforms.CenterCrop(min(image.size))

        cropped_image = transform(image)
        cropped_mask = transform(mask)

        resized_image = transforms.Resize((self.img_size, self.img_size),transforms.InterpolationMode.BILINEAR)(cropped_image)
        resized_mask= transforms.Resize((self.img_size, self.img_size), transforms.InterpolationMode.NEAREST)(cropped_mask)
        return resized_image, resized_mask

    def __len__(self):
        return len(self.images)

    def get(self, idx):
        # Load image
        img_name = os.path.join(self.image_dir, self.images[idx])
        image = Image.open(img_name)#.convert('RGB')

        annotation_name = os.path.join(self.annotation_dir, self.images[idx].replace('.jpg', '.png'))
        annotation = Image.open(annotation_name)

        ids = np.unique(np.array(annotation))
        labels = [self.digit_to_object_mapping[id] for id in ids]

        return image, annotation, labels

    def __getitem__(self, idx):
        # Load image
        img_name = os.path.join(self.image_dir, self.images[idx])
        image = Image.open(img_name)#.convert('RGB')

        # Load annotation
        annotation_name = os.path.join(self.annotation_dir, self.images[idx].replace('.jpg', '.png'))
        annotation = Image.open(annotation_name)

        image, mask = self.resize(image, annotation)
        mask = np.array(mask)
        mask += 1
        mask[mask==256] = 0


        # Indexed masks
        ids = np.unique(mask)
        ids = [id for id in ids if id != 0]
        nonempty_masks = [np.full(mask.shape, id) * (mask==id) for id in ids]
        nonempty_masks = sorted(nonempty_masks, key=lambda x: np.sum(x!=0), reverse=True)

        # Get ids and labels
        ids = [np.unique(mask)[-1] for mask in nonempty_masks]
        labels = [self.digit_to_object_mapping[id] for id in ids]

        # Convert to binary masks
        nonempty_masks = [(mask != 0).astype(float) for mask in nonempty_masks]

        sample = {'image': image, 'annotation': nonempty_masks, 'labels': labels, 'ids': ids}

        return sample

"""## Label Model"""

class CLIPLang_xatten(nn.Module):

    '''
    This class is for detecting most salient object
    in the scene as an auto-regressive task
    '''

    def __init__(self, nhead=4, nencoder=4, ndecoder=4, clip_version="openai/clip-vit-base-patch16"):
        super().__init__()

        vision_model = CLIPVisionModelWithProjection.from_pretrained(clip_version)
        text_model = CLIPTextModelWithProjection.from_pretrained(clip_version)

        # vision model
        self.vision_encoder = vision_model.vision_model
        self.vision_projector = vision_model.visual_projection

        # text model
        self.text_model = text_model.text_model
        self.text_projector = text_model.text_projection

        # internal dimensions
        self.d_text = self.text_model.embeddings.token_embedding.weight.shape[1]
        self.d_image = self.vision_encoder.embeddings.position_embedding.weight.shape[1]

        # Learnable embeddings
        self.query_embeddings = nn.Embedding(20, self.d_text)
        self.query_embeddings.weight.data.normal_(mean=0, std=0.02)

        # transformer for next label prediction
        self.decoder = nn.Transformer(
            d_model = self.d_text,
            nhead = nhead,
            num_encoder_layers = nencoder,
            num_decoder_layers = ndecoder,
            activation = nn.GELU(),
            batch_first = True
        )


    def to_embedding(self, input_ids):
        # index -> text_embeddings
        return self.text_model.embeddings.token_embedding(input_ids)

    def position_encode(self, embeddings):
        index_tensor = torch.arange(embeddings.shape[1]).repeat(embeddings.shape[0], 1).to(embeddings.device)
        return embeddings + self.text_model.embeddings.position_embedding(index_tensor)

    def visual_forward(self, pixel_values, output_hidden_states=False):
        # image -> text_embeddings
        return self.vision_encoder(pixel_values, output_hidden_states=output_hidden_states)

    def text_forward(self, text_embeddings):
        # text_embedding -> text_embeddings
        return self.text_model.encoder(text_embeddings)

    def decoder_forward(self, img_src, txt_tgt):
        # Send image seq and text seq to lang model
        return self.decoder(
            img_src,
            txt_tgt,
        )

    def cond_forward(self, pixel_values, output_hidden_states = False):
        # Get visual features
        visual_outputs = self.visual_forward(pixel_values, output_hidden_states=output_hidden_states)
        visual_features = self.vision_projector(visual_outputs.last_hidden_state)

        # Remove the [cls] token
        visual_features = visual_features[:, 1:, :]

        # Initialize query tokens
        index_tensor = torch.arange(20).repeat(pixel_values.shape[0], 1).to(pixel_values.device)
        query_tokens = self.query_embeddings(index_tensor)
        query_tokens = self.position_encode(query_tokens)

        # Decode texts
        text_pred = self.decoder_forward(visual_features, query_tokens)

        # Return both text embeddings and visual activations
        if output_hidden_states:
            return text_pred, [visual_outputs.hidden_states[i] for i in (3, 6, 9)]

        # Get text embeddings
        return text_pred

    def forward(self, pixel_values, output_hidden_states = False):

        if output_hidden_states:
            text_pred, hidden_states = self.cond_forward(pixel_values, output_hidden_states=output_hidden_states)
            return text_pred, hidden_states

        return self.cond_forward(pixel_values)

"""## Segmentation Model"""

class AutoSeg(nn.Module):
    def __init__(self, d_reduce=64, nhead=4, nencoder=4, ndecoder=4):
        super().__init__()

        self.encoders = CLIPLang_xatten(nhead=nhead, nencoder=nencoder, ndecoder=ndecoder)
        self.reduces = nn.ModuleList([
            nn.Linear(self.encoders.d_image, d_reduce) for _ in range(3)
        ])
        self.film_mul = nn.Linear(self.encoders.d_text, d_reduce)
        self.film_add = nn.Linear(self.encoders.d_text, d_reduce)

        self.decoder = nn.ModuleList([
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=d_reduce,
                    nhead=nhead,
                    dim_feedforward=2048,
                    dropout=0.1,
                    activation=nn.GELU(),
                    batch_first=True,
                ),
                num_layers=1,
            )
            for _ in range(3)
        ])

        self.mask_head = nn.Sequential(
            nn.Conv2d(d_reduce, d_reduce, kernel_size=(3, 3), padding=(1, 1), padding_mode="replicate"),
            nn.GELU(),
            nn.ConvTranspose2d(d_reduce, d_reduce//2, kernel_size=(4, 4), stride=(4, 4)),
            nn.GELU(),
            nn.ConvTranspose2d(d_reduce//2, 1, kernel_size=(4, 4), stride=(4, 4))
        )

    def forward(self, pixel_values):
        # Get text embeddings
        lang_output, hidden_states = self.encoders.cond_forward(pixel_values, output_hidden_states=True)

        # Image sequence size
        self.image_seq_size = int(np.sqrt(hidden_states[0].shape[1]))

        masks = []
        for i, batch_embeddings in enumerate(lang_output.permute(1, 0, 2)):
            a  = None
            for hs, block, reduce in zip(hidden_states, self.decoder, self.reduces):
                hs = hs.permute(1, 0, 2)
                if a is None:
                    a = reduce(hs)
                else:
                    a = a + reduce(hs)

                a = a * self.film_mul(batch_embeddings) + self.film_add(batch_embeddings)
                a = block(a)

            a = a[1:].permute(1, 2, 0)
            a = a.view(a.shape[0], a.shape[1], self.image_seq_size, self.image_seq_size)
            a = self.mask_head(a)
            masks.append(a)

        masks = torch.cat(masks, dim=1)
        return masks, lang_output

"""## Define the collate function"""

def collate_fn_factory(processor, label_embeddings, max_size=20):

    def collate_fn(batch):
        size = processor.image_processor.size['shortest_edge'] #224
        transform = transforms.ToTensor()

        # Preprocess pixel values
        images = [each['image'] for each in batch]
        batch_pixel_values = processor(None, images=images, return_tensors='pt')['pixel_values']

        # Preprocess labels
        ids = torch.full((len(batch), max_size), 0)
        ids[:, :max_size] = torch.tensor([each['ids'][:max_size] + [0] * (max_size - len(each['ids'])) for each in batch])

        # Preprocess masks
        batch_masks = np.stack([
            np.stack(each['annotation'][:max_size] + [np.zeros((size, size))] * (max_size - len(each['annotation'])) )
            for each in batch
        ])
        batch_masks = torch.tensor(batch_masks[:, :max_size])

        return {
            "pixel_values": batch_pixel_values,
            "masks": batch_masks.type(torch.float32),
            "ids": ids.type(torch.long)
        }

    return collate_fn

"""## Define the CLIP loss"""

def clip_loss(pred_features, target_features, temperature=0.07):
    """
    Compute the CLIP loss between image and text features.

    Parameters:
    - image_features: A tensor of shape (batch_size, feature_dim) containing the image features.
    - text_features: A tensor of shape (batch_size, feature_dim) containing the text features.
    - temperature: A scalar temperature parameter.

    Returns:
    - The CLIP loss.
    """
    # Normalize features
    pred_features = F.normalize(pred_features, dim=-1)
    target_features = F.normalize(target_features, dim=-1)

    # Compute similarity matrix
    similarity = torch.matmul(pred_features, target_features.T) / temperature

    # Image-to-text and text-to-image loss
    loss = F.cross_entropy(similarity, torch.arange(len(pred_features)).to(pred_features.device))

    # Symmetric loss
    return loss

def anchor_clip_loss(pred_features, target_features, ground_truth, temperature=0.07):
    """
    Compute the CLIP loss between image and text features.

    Parameters:
    - image_features: A tensor of shape (batch_size, feature_dim) containing the image features.
    - text_features: A tensor of shape (batch_size, feature_dim) containing the text features.
    - temperature: A scalar temperature parameter.

    Returns:
    - The CLIP loss.
    """
    # Normalize features
    pred_features = F.normalize(pred_features, dim=-1)
    target_features = F.normalize(target_features, dim=-1)

    # Compute similarity matrix
    similarity = torch.matmul(pred_features, target_features.T) / temperature

    # Image-to-text and text-to-image loss
    loss = F.cross_entropy(similarity, torch.arange(len(pred_features)).to(pred_features.device))

    # Symmetric loss
    return loss

"""## Training Pipeline"""

device = 0

# Define dataset dir
dataset_dir = "/scratch/t.tovi/datasets/"

# Create dataset object
data = COCOStuffDataset(
    dataset_dir+"COCO_stuff_images/train2017",
    dataset_dir+"COCO_stuff_annotations/train2017",
    img_size=224
)

lang_model = CLIPLang_xatten()
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")

# Get loss query table

labels = data.digit_to_object_mapping

label_indices = list(data.digit_to_object_mapping.keys())
label_text = ["a photo of " + data.digit_to_object_mapping[each] for each in label_indices]
label_indices = processor(label_text, padding=True, return_tensors='pt')['input_ids']

with torch.no_grad():
    label_embeddings = lang_model.text_model(label_indices)["pooler_output"]
    eos_embedding = lang_model.text_model(torch.tensor([processor.tokenizer.eos_token_id]))["pooler_output"]
    bos_embedding = lang_model.text_model(torch.tensor([processor.tokenizer.bos_token_id]))["pooler_output"]

label_embeddings.requires_grad_(False)
eos_embedding.requires_grad_(False)
bos_embedding.requires_grad_(False)

reverse_mapping = {v: k for k, v in data.digit_to_object_mapping.items()}

# Get the collate function

collate_fn = collate_fn_factory(processor, label_embeddings)

# Create batch data loader
data_loader = DataLoader(data, batch_size=64, collate_fn=collate_fn, num_workers=4, shuffle=True)

# Initialize the model
model = AutoSeg(d_reduce=128)
m = nn.Sigmoid()
model.to(device)

# Freeze all
model.requires_grad_(False)

# # Unfreeze decoders
encoder_params = [
    model.encoders.query_embeddings,
    model.encoders.decoder,
]

decoder_params = [
    model.reduces,
    model.film_mul,
    model.film_add,
    model.decoder,
    model.mask_head
]

for param in encoder_params + decoder_params:
    param.requires_grad_(True)

# Define training parameters
lr_encoder = 1e-4
lr_decoer = 1e-4
alpha = 0.08
temperature = 0.08
num_epochs = 16

# Optimizer
optim = Adam(
    [
        {'params': param.parameters(), "lr" : lr_encoder}
        for param in encoder_params
    ] +\
    [
        {'params': param.parameters(), "lr" : lr_decoer}
        for param in decoder_params
    ]
)

scheduler = CosineAnnealingLR(optim, T_max=len(data_loader), eta_min=1e-6)

# Loss
mask_objective = nn.BCELoss()
lang_objective = nn.CrossEntropyLoss()

"""## Train"""
#model.load_state_dict(torch.load("/scratch/t.tovi/autoseg_v0.3"))

label_embeddings = label_embeddings.to(0)
label_embeddings = F.normalize(label_embeddings, dim=-1)

count = 0
for _ in range(num_epochs):
    batch_loss = 0
    batch_l1 = 0
    batch_l2 = 0
    for batch in data_loader:
        # Prepare data
        pixel_values = batch['pixel_values'].to(device)
        masks = batch['masks'].to(device)
        ids = batch['ids'].to(device)

        mask_logits, pred_embeddings = model(pixel_values)
        pred_embeddings = F.normalize(pred_embeddings, dim=-1)
        label_logits = pred_embeddings @ label_embeddings.T / temperature

        # Compute loss
        mask_prob = m(mask_logits)
        l1 = mask_objective(mask_prob, masks)
        l2 = alpha * lang_objective(label_logits.permute(0, 2, 1), ids)

        # Total loss
        loss = l1 + l2

        loss.backward()
        optim.step()
        optim.zero_grad()

        batch_loss += loss.detach().cpu().item()
        batch_l1 += l1.detach().cpu().item()
        batch_l2 += l2.detach().cpu().item()

        scheduler.step()

        if (count+1) % 64 == 0:
            print(f"Avrage batch loss: {batch_loss / 64}, {batch_l1 / 64}, {batch_l2 / 64}")
            batch_loss = 0
            batch_l1 = 0
            batch_l2 = 0

        count += 1


    print("One training epoch done")
    torch.save(model.state_dict(), "/scratch/t.tovi/autoseg_v0.3_128")
